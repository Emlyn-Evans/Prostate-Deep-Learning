# -*- coding: utf-8 -*-
"""metrics_for_performance_evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P3dyyTLVTc9_HP5csoqltVpSPCzIMQZT

# Metrics for performance evaluation

Exercise for Dalyell group week 6, Sep 2019

Yu Sun, yu.sun@sydney.edu.au

University of Sydney

What we will do:
* Generate some dummy data and train a classifier. Here, the machine learning algorithm is not a deep learning method (a support vector machine, or SVM). Don't worry about the details of SVMs. The focus is what comes next, i.e. how we measure the performance, which is method-independent;
* Start with *accuracy* and look at its limitation;
* Move onto *sensitivity* and *specificity*;
* Finnally, introduce the receiver operating characteristics (ROC) curve, which is the standard way to report a model for binary classification. In researcher paper, there's usually an ROC curve for binary prediction. So make sure you understand how an ROC curve is constructed.

## Classifier training
In this part, we will create a learning task and train a model.

### Generate data for classification
"""

# Import the required libraries
import sklearn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split

# Generate some dummy data
X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, 
                           n_informative=2, random_state=10, 
                           n_clusters_per_class=1)

# Visualise the data
# First we define a function to do the scatter plot
def plotData(x, y, title='Title'):
  'Scatter plot of the x, colour indicated by y'
  import matplotlib.pyplot as plt
  import matplotlib.patches as mpatches
  plt.style.use('ggplot')
  plt.scatter(x[:,0], x[:, 1], 
              c=y, 
              cmap=ListedColormap(['orange', 'darkcyan']), # for 0 and 1
              edgecolors='k',
              s=80)
  plt.xlabel('x1')
  plt.ylabel('x2')
  plt.title(title)
  patches = (mpatches.Patch(color='orange', label='Negative'),
             mpatches.Patch(color='darkcyan', label='Positive'))
  plt.legend(handles=patches)
  plt.show()

plotData(X, y, 'Generated data for classification')

"""### Data partitioning"""

# Split the data into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)

# Check the dimensions
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""### Model fitting"""

# Train a support vector machine (SVM) model
from sklearn.svm import SVC
svm_model = SVC(gamma=1, C=1)

# Train the model
svm_model.fit(X_train, y_train)

"""### Performance evalution

#### Apply the model on the test data
"""

# Predict on the test data
svm_pred = svm_model.predict(X_test)
print('Prediction (SVM): ', svm_pred)
print('Ground truth: ', y_test)

"""We need to choose a proper metric to *quantitatively* measure the performance of the model. The most straightfoward metric is **accuracy**.

#### Accuracy
Defined as *the percentage* that the prediction agrees with the ground truth (`y_test`).

**Exercise Q1**

Given the prediction `svm_pred` and the ground truth `y_test`, write some Python code to compute accuracy.
"""

# Q1

# Finding how many were correct
correct = 0
for i in range(len(y_test)):
  if svm_pred[i] == y_test[i]:
    correct += 1

# Computing the accuracy
accuracy = (correct / len(y_test)) * 100

# Printing accuracy
print("The model's accuracy: %.2f%%" % accuracy)

"""**Compare** your answer with my answer (at the end of the document). Explore the different component in the expression. What does each part do? For example: what does it mean for `svm_pred == y_test`? Use `print()` to inspect each component."""

'''Creates a boolean list containing True/False which satisfy the 
condition that svm_pred equals y_test at that particular position'''
boolean_list = svm_pred == y_test
print(boolean_list)

'''Sums only the 'True' cells as 1.
I.e all cells where svm_pred == y_test'''
num_correct = sum(boolean_list)
print(num_correct, "correct predictions")

'''Finds the length of the numpy array svm_pred.
y_test has the same length since this is the number of tests'''
num_of_tests = svm_pred.size
print(num_of_tests, "predictions") 
print(y_test.size, "tests")

# Calculates the accuracy as a decimal
accuracy = num_correct / num_of_tests
print(accuracy, "is the accuracy")
print(accuracy == sum(svm_pred == y_test) / svm_pred.size)

"""**Think**: for a learning task of imbalanced data (say 90% positive, 10% negative). If a model simply predict everything as positive, what's the accuracy? <font color='red'> *It would be 90%* </font> In another word, is accuracy suitable for all learning scenarios? <font color='red'> *I guess not* </font>

The answer to that leads to sensitivity and specificity analysis.

#### Sensitivity and Specificity
Write a function `calSS()` to calculate sensitivity and specificity given their definitions: $$Sensitivity = TP / (TP+FN)$$ $$Specificity = TN / (TN+FP)$$

$TP$: true positive;
$TN$: true negative;
$FP$: false positive;
$FN$: false negative.

**Exercise Q2**

Check the [definitions](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) of $TP$, $TN$, $FP$ and $FN$ (take a look at the "Worked Example"). Write a function `calSS()` that given the prediction and ground truth, returns the sensitivity and specificity pair.
"""

# Q2
'''
Calculates the sensitivity and specificity of a binary classification test.

@para  `prediction`   the numpy array of predictions given a test
       `ground_truth` the numpy array of true test answers
@return  `sens`       the sensitivity of the model (proportion of TP identified)
         `spec`       the specificity of the model (proportion of TN identified)
'''
def calSS(prediction, ground_truth):
  
  TP = sum(np.logical_and(prediction == 1, ground_truth == 1)) # True Positive
  FP = sum(np.logical_and(prediction == 1, ground_truth == 0)) # False Positive
  TN = sum(np.logical_and(prediction == 0, ground_truth == 0)) # True Negative
  FN = sum(np.logical_and(prediction == 0, ground_truth == 1)) # False Negative
  
  sens = TP / (TP + FN) # Sensitivity
  spec = TN / (TN + FP) # Specificity
  
  return sens, spec

"""Check your answers, and then use the `calSS()` function."""

# Calculate the results on our previous model
print("Sensitivity: %.2f, specificity: %.2f" % calSS(svm_pred, y_test))

"""Back to the last question, in an imbalanced dataset (90% positive, 10% negative), if a model simply output everything as positive, it will have a high sensitivity. However, it won't have a high specificity. *In another word, sensitivity and specifity give you a 'stereo' overview of the model.*

We will use (sen, spe) as a short hand notation for a sensitivity and specificity pair.

#### Receiver operating characteristics (ROC) analysis

Some models such as the logistic regression output a probabilistic output rather than a binary output. Now we train a logistic regression using the same training data.

The logistic regression can be considered as a neural network that has no hidden layers and uses the sigmoid function as the activation function. The output of the logistic regression resembles what you'll get from a convolutional neural network (CNN). Again, don't worry about the method itself, but focus on how we process the output.
"""

# Initiate the logistic regression model
lor_model = sklearn.linear_model.LogisticRegression(solver='lbfgs')
# Train the model
lor_model.fit(X_train, y_train)
# Test the model
lor_pred = lor_model.predict_proba(X_test)
# Take a look at the first ten
print(lor_pred[:10, :])

"""For each row in the output, the two numbers give the probability that each test sample falls into the two categories (the same as a CNN). Notice, they sum up to 1.0 for each row.

An issue arises as we cannot directly compute the (sen, spe) now. But what we can do is to first threshold the value, i.e. turn each row to a binary decision, then compute the (sen, spe). 

We will define a function `thres()` to do that.
"""

def thres(pred, t):
    '''
    Convert a continuous input to binary output using a threshold.
    Input:
        pred - the prediction vector
        t - a threshold
    Output:
        A binary vector
    '''
    return (pred >= t) * 1

# Test it on the logistic regression output using a threshold of 0.5
lor_pred_binary = thres(lor_pred[:, 1], t=0.5)
print(lor_pred_binary)

# Now we can compute the (sen, spe)
print("Sensitivity: %.2f, specificity: %.2f" % calSS(lor_pred_binary, y_test))

"""The result above is dependent on the *threshold*. For a given threshold, we can convert the continuous output into binary. Then we can compute the sensitivity and specificity.

One might argue, why we should choose 0.5 as the threshold. What if we create **a series of thresholds** , compute **the corresponding (sen, spe)** for each threshold and develop a metric based on the (sen, spe) series?
"""

# Create a series of thresholds
ts = range(0, 11)

# Create variables to hold the (sen, spe) pairs
lor_pred_sen = []
lor_pred_spe = []

# Iterate through each threshold and compute the corresponding (sen, spe) pairs
for eachT in ts:
    sen, spe = calSS(thres(lor_pred[:, 1], t=eachT/10.0), y_test)
    lor_pred_sen.append(sen)
    lor_pred_spe.append(spe)
    
# Print the result
for i in range(0, len(ts)):
    print('Thresholded at %s, sensitivity: %.3f, specificity: %.3f' % 
          (i/10.0, lor_pred_sen[i], lor_pred_spe[i]))

"""Plot the (sen, spe) points, what do we get? This will give the ROC curve."""

# Plot the (sen, spe) pairs
import numpy as np
plt.step(x=np.array(lor_pred_spe), y=np.array(lor_pred_sen), where='post')
plt.xlabel('Specificity')
plt.ylabel('Sensitivity')
plt.title('Receiver Operating Charcteristics Curve')

"""By convention, the x-axis is '1-specificity'."""

# Change the x to (1-spe) for the ROC curve
plt.step(x=1-np.array(lor_pred_spe), y=np.array(lor_pred_sen), where='post')
plt.xlabel('1-Specificity')
plt.ylabel('Sensitivity')
plt.title('Receiver Operating Charcteristics Curve')

"""This is a typically ROC curve, although most models won't reach such high performance. To develop a new metric, people use the area under the ROC curve (AUC of ROC, or AUROC) as a measure for model performance. The AUC is a number from 0.5 (random guess) to 1.0 (perfect model). In this example, it's very close to 1.0.

For more details on the ROC curve, please read this [article](http://medind.nic.in/ibv/t11/i4/ibvt11i4p277.pdf) (optional).

## Summary

In summary, in terms of robustness:
$$ROC >  (sen, spe) > accuracy$$

### Suggested answers
"""

# Q1
# Compute the accuracy for the SVM model
sum(svm_pred == y_test) / svm_pred.size

# Q2
# A function to calcualte sensitivity and specificity given the prediction and 
#  the ground truth
def calSS(pred, truth):
    '''
    Compute the sensitivity and specificity of a prediction given the ground truth
    Input:
        pred - predicted outcome (binary)
        truth - ground truth (binary)
    Return: 
        A tuple of sensitivity and specificity value
    '''
    tp = np.logical_and(pred==1, truth==1) # True Positivee (TP)
    fp = np.logical_and(pred==1, truth==0) # False Positive (FP)
    tn = np.logical_and(pred==0, truth==0) # True Negative (TN)
    fn = np.logical_and(pred==0, truth==1) # False Negative (FN)

    sen = tp.sum() / (tp.sum() + fn.sum())
    spe = tn.sum() / (tn.sum() + fp.sum())
    return sen, spe